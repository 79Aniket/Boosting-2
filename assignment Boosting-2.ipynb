{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0006e8e1-8dfc-4326-a925-00b124496207",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee83c25-2c0f-4fa8-ab04-c95941c76570",
   "metadata": {},
   "source": [
    "## Gradient boosting regression (GBR) is a machine learning algorithm that can be used to predict continuous values. It works by building a sequence of weak learners, which are typically decision trees, and then combining them to produce a strong learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326caa2d-05e9-44b1-ab3d-99d3b445b099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b108200-5e96-4e52-a5a6-f752598d06d3",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e44462-bf42-4fe7-8b6a-5f0870c19102",
   "metadata": {},
   "source": [
    "## A weak learner in gradient boosting is a simple machine learning model that is only slightly better than random guessing. Gradient boosting works by iteratively training weak learners on the residuals of the previous weak learners, and then combining them to produce a strong learner.Weak learners are typically decision trees, but other types of models can also be used. Decision trees are a good choice for weak learners because they are simple to train and interpret, and they can be used to learn both linear and non-linear relationships between the features and the target variable.\n",
    "## Gradient boosting uses a gradient descent algorithm to minimize the loss function of the ensemble model. The loss function is a measure of how well the ensemble model is predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75bd09-a70e-4504-aac7-295a10a91f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c1afa07-0262-4da6-81be-e61fc244d8f0",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435f416-3786-4ebd-8ef9-6ff79b3ed1a2",
   "metadata": {},
   "source": [
    "## The intuition behind the gradient boosting algorithm is to gradually build up a strong learner from a series of weak learners. Each weak learner is trained on a different weighted version of the training data, and the weights are adjusted after each iteration to give more importance to the data points that the previous weak learners were unable to learn correctly.\n",
    "\n",
    "## The gradient boosting algorithm uses a gradient descent algorithm to minimize the loss function of the ensemble model. The loss function is a measure of how well the ensemble model is predicting the target variable.\n",
    "\n",
    "## At each iteration, the gradient boosting algorithm trains a new weak learner to minimize the loss function on the residuals of the previous weak learners. The residuals are the difference between the predicted values of the previous weak learners and the actual values of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaa6d0-90af-40d5-a800-44fda06328a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690141b-ec13-4036-8e1c-bf854bf7af07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
